{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Effects of a Student's Circusmtances and Decisions on His or Her Grades\n",
    "\n",
    "##Jai Naidu\n",
    "##1/17/17\n",
    "##Period 8\n",
    "##Ms. Anderson, Lab 3\n",
    "##Doc: https://www.kaggle.com/uciml/student-alcohol-consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this lab to is to answer the overarching question: How do circumstances and decisions affect a student’s grades? Using the decision trees machine learning algorithm, I aim to predict a student’s grades based on several different circumstances (family relationship, tutoring, etc.) and decisions (alcohol consumption, study time, etc.). After doing so, I will evaluate the how influential each decision or circumstance is on the student’s grades. This will enable me to determine if a student can overcome his circumstances, and how hard or easy it is to do so based on his decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 11)\n",
      "[[  2.   0.   1. ...,   1.   3.   6.]\n",
      " [  2.   0.   1. ...,   1.   3.   4.]\n",
      " [  2.   3.   0. ...,   3.   3.  10.]\n",
      " ..., \n",
      " [  1.   3.   1. ...,   3.   3.   3.]\n",
      " [  1.   0.   1. ...,   4.   5.   0.]\n",
      " [  1.   0.   1. ...,   3.   5.   5.]]\n",
      "[ 0.  0.  0.  1.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.\n",
      "  0.  0.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  0.  1.\n",
      "  1.  0.  1.  1.  0.  1.  1.  1.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.\n",
      "  0.  1.  1.  0.  0.  1.  0.  0.  1.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
      "  0.  1.  0.  0.  1.  0.  1.  0.  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.\n",
      "  1.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.\n",
      "  1.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  1.  1.\n",
      "  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  1.  1.  0.  0.\n",
      "  0.  0.  0.  1.  0.  1.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.\n",
      "  0.  1.  1.  0.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.  1.  1.  1.  0.\n",
      "  1.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0.  1.\n",
      "  0.  0.  1.  1.  1.  0.  1.  1.  0.  1.  0.  1.  1.  0.  0.  1.  0.  0.\n",
      "  0.  0.  1.  0.  1.  1.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0.\n",
      "  0.  1.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  1.  1.\n",
      "  1.  0.  1.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  1.\n",
      "  1.  1.  1.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  1.  0.\n",
      "  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0.  1.\n",
      "  1.  1.  0.  1.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0.  1.  0.  1.  0.\n",
      "  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#open the file\n",
    "raw_data = open(\"studentmath.csv\")\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "#'shape' is the dimensions of the matrix\n",
    "print(dataset.shape)\n",
    "# separate the data from the target attributes\n",
    "X = dataset[:,0:10]\n",
    "y = dataset[:,10]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.545454545455\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=.5)\n",
    "\n",
    "my_classifier=tree.DecisionTreeClassifier();\n",
    "my_classifier=my_classifier.fit(X_train, y_train)\n",
    "predictions=my_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc: http://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/\n",
    "Doc: http://www.jmlr.org/proceedings/papers/v28/bi13.pdf\n",
    "Doc:http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "After initially uploading the file and tailoring it to fit a machine learning algorithm, I did a few accuracy tests using the machine learning algorithms that we have learnt. My initial accuracy tests were undesirable. My accuracy ranged from 10 to 20 percent. With this kind of accuracy, the code really won't tell us anything useful. First of all, I identified the issue as overfitting. The raw dataset I was given (after changing strings to integers) had a lot of \"noise\"; data that fluctuates from the general patter of the data. So I tried several methods to improve accuracy. My first thought was to reduce the amount of features. Some of the features were unhelpful, and in most cases, had no correlation to the grades, therefore, lowering the accuracy of the machine learning algorithm. I got rid of several of the features including, location, age, gender, etc. After doing so the accuracy percentage benefitted minimally (up to around 25 percent accuracy). Regardless, the percentage did go up, so I left the features as they were (10 features). Upon gaining more understanding of machine learning, it was made clear to me that the bulk of the issue was in the labels. The output of the algorithm could have been anywhere from 1 to 20. After doing some research on how to solve an issue like this, I came to the collusion that reducing the labels was my best bet. While I did find other methods to effectively use several labels (exploitation of label correlations and label hierarchy), these concepts were too complicated to apply within a week or two with minimal coding experience. So I reduced the amount of labels, first down to 10, then down to 3, and finally down to 2. Every time I lowered the amount of labels the accuracy benefitted greatly. From an unusable 12 percent accuracy, I was able to get to around a 50-60 percent accuracy consistently. While the accuracy is still undesirable, it is usable. More importantly, it tells us something about the data. Cleary, the data doesn't have a lot of correlation. Given the features that I utilized (educational support, family relationship, study time, absences, failures, health, motivation, free time, and alcohol consumption) could almost all be effected by circumstance, this accuracy tells us that a student's circumstance may not affect their performance all that much. However, this not a 100 percent concrete conclusion. In order to prove this conclusion, a larger amount of data, and a larger variety of features would be necessary. However, ultimately, I continued analyzing this data set since the accuracy of the machine learning algorithm was usable (keeping in mind that my findings are not completely accurate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jai\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=.5)\n",
    "\n",
    "from sklearn import tree\n",
    "tree_classifier = tree.DecisionTreeClassifier();\n",
    "tree_classifier = tree_classifier.fit(X_train, y_train)\n",
    "tree_predictions = tree_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X,y)\n",
    "\n",
    "print(clf.predict([2,2,1,0,1,1,3,3,51,50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran some breif tests to see how often the code would mess up here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "import random\n",
    "import statistics as stat\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy Percentage Using study time as the only determining factor:\n",
      "\tDecision Tree: 55.26%\n",
      "Average Accuracy Percentage Using failures as the only determining factor\n",
      "\tDecision Tree: 63.86%\n",
      "Average Accuracy Percentage Using paid educational support as the only determining factor\n",
      "\tDecision Tree: 50.62%\n",
      "Average Accuracy Percentage Using Extracurriculars Intake as the only determining factor\n",
      "\tDecision Tree: 50.58%\n",
      "Average Accuracy Percentage Using Motivation as the only determining factor\n",
      "\tDecision Tree: 56.39%\n",
      "Average Accuracy Percentage Using Family Relationship as the only determining factor\n",
      "\tDecision Tree: 48.56%\n",
      "Average Accuracy Percentage Using free time as the only determining factor\n",
      "\tDecision Tree: 52.18%\n",
      "Average Accuracy Percentage Using Alcohol Intake as the only determining factor\n",
      "\tDecision Tree: 55.76%\n",
      "Average Accuracy Percentage Using health as the only determining factor\n",
      "\tDecision Tree: 51.55%\n",
      "Average Accuracy Percentage Using absences as the only determining factor\n",
      "\tDecision Tree: 51.52%\n"
     ]
    }
   ],
   "source": [
    "#Doc: Jacob Richey\n",
    "\n",
    "forgraph = []\n",
    "\n",
    "X = dataset[:,0:10]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using study time as the only determining factor:\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")\n",
    "\n",
    "X = dataset[:,1:2]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using failures as the only determining factor\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")\n",
    "\n",
    "X = dataset[:,2:3]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using paid educational support as the only determining factor\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")\n",
    "\n",
    "X = dataset[:,3:4]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using Extracurriculars Intake as the only determining factor\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")\n",
    "\n",
    "X = dataset[:,4:5]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using Motivation as the only determining factor\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")\n",
    "\n",
    "X = dataset[:,5:6]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using Family Relationship as the only determining factor\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")\n",
    "\n",
    "X = dataset[:,6:7]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using free time as the only determining factor\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")\n",
    "\n",
    "X = dataset[:,7:8]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using Alcohol Intake as the only determining factor\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")\n",
    "\n",
    "X = dataset[:,8:9]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using health as the only determining factor\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")\n",
    "\n",
    "X = dataset[:,9:10]\n",
    "y = dataset[:,10]\n",
    "classifieraccuracy = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "    my_classifier = tree.DecisionTreeClassifier()\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    classifieraccuracy.append(accuracy_score(predictions, y_test)*100)\n",
    "    \n",
    "print(\"Average Accuracy Percentage Using absences as the only determining factor\")\n",
    "print(\"\\tDecision Tree:\", str(round(stat.mean(classifieraccuracy), 2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAEZCAYAAACuFJ7kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25c15e32898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importance = [55.3, 64.02, 49.33, 51.2, 55.69, 48.7, 51.96, 55.78, 50.64, 51.71]\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "plt.title(\"Percent of Accuracy of Machine Learning Algorithm Using each Individual Feature Alone\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Percent of Accuracy\")\n",
    "plt.bar(numbers, importance, color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference:\n",
    "1 = Study Time\n",
    "2 = Failures\n",
    "3 = Paid Educational Support\n",
    "4 = Extracurricular Activities\n",
    "5 = Motivation (Desire to go to college)\n",
    "6 = Family Relationship\n",
    "7 = Free Time\n",
    "8 = Alcohol Consumption on the Weekends\n",
    "9 = Health (How healthy the student is)\n",
    "10 = Absences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: My accuracy using only failures was initially the lowest, however, I found a small error in the code that changed the data.\n",
    "\n",
    "The results that I ended up with were more comprehensive than I expected from a 50-60 percent accurate machine learning algorithm. For clarity, I split the data into a few tiers of accuracy percentage (or \"importance\"). First, tier 1: according to the results, the most determining factor of a student’s grade is failures. I used this feature mostly to test the accuracy of the data because it only makes sense that a student that fails his classes has bad grades. Secondly, tier 2: study time, motivation, and alcohol consumption are a little less, but still very \"important\" or influential factors in determining a student’s grades. This doesn't come as much of a surprise. According to the results, the more you study the better your grades (this is pretty self-explanatory); the less you drink the better your grades are (alcohol may serve as a distraction from grades); the more motivated you are the better your grades are (student's need good grades to get into college, so the correlation here is clear). Before I get into the third tier of \"importance\", it is important to note that tier 2 and 3 don't vary much in accuracy percentages. The tier 3 determining factors of a student’s grades are: paid educational support, extracurricular activities, family relationship, free time, health, and absences. In this case, paid educational support and family relationship are least \"important\". The rest of the tier 3 factors follow in a relative close second as the least \"important\" deciding factor of a student's grades. All in all, this data tells us a few interesting things. We must first keep in mind that these determine factors of grades are all fairly similar in \"importance\" OVERALL. But we are taking a closer, more analytical look. Generally, speaking, the data tells us that circumstance is not important in determining a student’s grades. For one, look at the contrast between paid education support and the tier 1 \"importance\" factors. The data suggests that tutoring may not be as important as people believe; rather, it is mostly up to the student to get good grades. It is up to the student to be motivated, study and lot, and avoid drugs. Another thing that stood out was the fact that family relationship wasn't as \"important\" as other things. While it still affects a student’s grades it isn't as important as tier 1 factors. Speaking for the tier 2 and high tier 3 factors, these factors lie in the center of the data, they are not outliers, so there is not much to say about them. All we can say, is that they are important to determining a student's grades, however, maybe not quite as much as tier 1 factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, we were able to fish out an answer from the data. The data provided us with an overarching answer to our question. It is important, however, to notice that in order to derive this answer we had to look at the data under a microscope. The variations in \"importance\" don't vary all that much implying that all of these circumstances and decisions are similarly impactful to a student's grades, generally speaking. However, yes, circumstances do affect a student's grades, however, a student's motivation, study time, and ability to avoid drugs affect his or her grades more. In other words, if the student is willing to be disciplined, he can overcome his circumstances. Since, motivation, study time, and the ability to stay away from drugs have only a 2 percent higher accuracy (when the individual factor is used to calculate accuracy) than family relationship and paid education support, the data suggests that it may not be easy to overcome circumstances, however, it is quite possible."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
